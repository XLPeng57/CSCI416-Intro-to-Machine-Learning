{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for SMS spam classification\n",
    "\n",
    "\n",
    "Each line of the data file `sms.txt`\n",
    "contains a label---either \"spam\" or \"ham\" (i.e. non-spam)---followed\n",
    "by a text message. Here are a few examples (line breaks added for readability):\n",
    "\n",
    "    ham     Ok lar... Joking wif u oni...\n",
    "    ham     Nah I don't think he goes to usf, he lives around here though\n",
    "    spam    Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\n",
    "            Text FA to 87121 to receive entry question(std txt rate)\n",
    "            T&C's apply 08452810075over18's\n",
    "    spam    WINNER!! As a valued network customer you have been\n",
    "            selected to receivea £900 prize reward! To claim\n",
    "            call 09061701461. Claim code KL341. Valid 12 hours only.\n",
    "\n",
    "To create features suitable for logistic regression, code is provided to do the following (using tools from the ``sklearn.feature_extraction.text``):\n",
    "\n",
    "* Convert words to lowercase.\n",
    "* Remove punctuation and special characters (but convert the \\$ and\n",
    "  £ symbols to special tokens and keep them, because these are useful for predicting spam).\n",
    "* Create a dictionary containing the 3000 words that appeared\n",
    "  most frequently in the entire set of messages.\n",
    "* Encode each message as a vector $\\mathbf{x}^{(i)} \\in\n",
    "  \\mathbb{R}^{3000}$. The entry $x^{(i)}_j$ is equal to the\n",
    "  number of times the $j$th word in the dictionary appears in that\n",
    "  message.\n",
    "* Discard some ham messages to have an\n",
    "  equal number of spam and ham messages.\n",
    "* Split data into a training set of 1000 messages and a\n",
    "  test set of 400 messages.\n",
    "  \n",
    "Follow the instructions below to complete the implementation. Your job will be to:\n",
    "\n",
    "* Learn $\\boldsymbol{\\theta}$ by gradient descent\n",
    "* Plot the cost history\n",
    "* Make predictions and report the accuracy on the test set\n",
    "* Test out the classifier on a few of your own text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prep data\n",
    "This cell preps the data. Take a look to see how it works, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "\n",
    "from logistic_regression import logistic, cost_function, gradient_descent\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Preprocess the SMS Spam Collection data set\n",
    "#  \n",
    "#   https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "# \n",
    "# From Dan Sheldon\n",
    "\n",
    "numTrain    = 1000\n",
    "numTest     = 494\n",
    "numFeatures = 3000\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Open the file\n",
    "f = codecs.open('sms.txt', encoding='utf-8')\n",
    "\n",
    "labels = []    # list of labels for each message\n",
    "docs   = []    # list of messages\n",
    "\n",
    "# Go through each line of file and extract the label and the message\n",
    "for line in f:\n",
    "    l, d= line.strip().split('\\t', 1)\n",
    "    labels.append(l)\n",
    "    docs.append(d)\n",
    "\n",
    "# This function will be called on each message to preprocess it\n",
    "def preprocess(doc):\n",
    "    # Replace all currency signs and some url patterns by special\n",
    "    # tokens. These are useful features.\n",
    "    doc = re.sub('[£$]', ' __currency__ ', doc)\n",
    "    doc = re.sub('\\://', ' __url__ ', doc)\n",
    "    doc = doc.lower() # convert to lower\n",
    "    return doc\n",
    "\n",
    "\n",
    "# This is the object that does the conversion from text to feature vectors\n",
    "vectorizer = CountVectorizer(max_features=numFeatures, preprocessor=preprocess)\n",
    "\n",
    "# Do the conversion (\"fit\" the transform from text to feature vector. \n",
    "#   later we will also \"apply\" the tranform on test messages)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Convert labels to numbers: 1 = spam, 0 = ham\n",
    "y = np.array([l == 'spam' for l in labels]).astype('int')\n",
    "\n",
    "# The vectorizer returns sparse scipy arrays. Convert this back to a dense \n",
    "#   numpy array --- not as efficient but easier to work with\n",
    "X = X.toarray()\n",
    "m,n = X.shape\n",
    "\n",
    "# Add a column of ones\n",
    "X = np.column_stack([np.ones(m), X])\n",
    "\n",
    "# \n",
    "# Now massage and split into test/train\n",
    "# \n",
    "pos = np.nonzero(y == 1)[0]   # indices of positive training examples\n",
    "neg = np.nonzero(y == 0)[0]   # indices of negative training examples\n",
    "\n",
    "npos = len(pos)\n",
    "\n",
    "# Create a subset that has the same number of positive and negative examples\n",
    "subset = np.concatenate([pos, neg[0:len(pos)] ])\n",
    "\n",
    "# Randomly shuffle order of examples\n",
    "np.random.shuffle(subset)\n",
    "      \n",
    "X = X[subset,:]\n",
    "y = y[subset]\n",
    "\n",
    "# Split into test and train\n",
    "train = np.arange(numTrain)\n",
    "test  = numTrain + np.arange(numTest)\n",
    "\n",
    "X_train = X[train,:]\n",
    "y_train = y[train]\n",
    "\n",
    "X_test  = X[test,:]\n",
    "y_test  = y[test]\n",
    "\n",
    "# Extract the list of test documents\n",
    "test_docs = [docs[i] for i in subset[test]]\n",
    "\n",
    "# Extract the list of tokens (words) in the dictionary\n",
    "tokens = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regresion model\n",
    "Now train the logistic regression model. The comments summarize the relevant variables created by the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.07178391  0.20490575  0.52915622 ...  0.         -0.41953938\n",
      "  0.06853638]\n",
      "Cost function: 33.36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8626f371f0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X_train     contains information about the words within the training\n",
    "#             messages. the ith row represents the ith training message. \n",
    "#             for a particular text, the entry in the jth column tells\n",
    "#             you how many times the jth dictionary word appears in \n",
    "#             that message\n",
    "#\n",
    "# X_test      similar but for test set\n",
    "#\n",
    "# y_train     ith entry indicates whether message i is spam\n",
    "#\n",
    "# y_test      similar\n",
    "#\n",
    "\n",
    "m, n = X_train.shape\n",
    "\n",
    "theta = np.zeros(n)\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: \n",
    "#  - learn theta by gradient descent \n",
    "#  - plot the cost history\n",
    "#  - tune step size and # iterations if necessary\n",
    "iters = 10000\n",
    "theta, J_history = gradient_descent(X_train,y_train,theta,0.0001,iters)\n",
    "print(theta)\n",
    "print (\"Cost function: %.2f\" % cost_function(X_train, y_train, theta))\n",
    "\n",
    "axis = []\n",
    "for i in range(iters):\n",
    "    axis.append(i)\n",
    "plt.plot(axis, J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "Use the model fit in the previous cell to make predictions on the test set and compute the accuracy (percentage of messages in the test set that are classified correctly). You should be able to get accuracy above 95%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9635627530364372.\n"
     ]
    }
   ],
   "source": [
    "m_test, n_test = X_test.shape\n",
    "\n",
    "pred = logistic(X_test.dot(theta.T))\n",
    "# print(pred)\n",
    "# YOUR CODE HERE\n",
    "#  - use theta to make predictions for test set\n",
    "#  - print the accuracy on the test set---i.e., the precent of messages classified correctly\n",
    "\n",
    "\n",
    "fit = [] \n",
    "for x in logistic(X_test.dot(theta.T)):\n",
    "    if x < 0.5:\n",
    "        fit.append(0)\n",
    "    else:\n",
    "        fit.append(1)\n",
    "data = []\n",
    "num = 0\n",
    "for x in y_test:\n",
    "    if x == 1:\n",
    "        data.append(1)\n",
    "    if x == 0:\n",
    "        data.append(0)\n",
    "        \n",
    "for x,y in zip(fit,data):\n",
    "    if x == y:\n",
    "        num += 1\n",
    "acc = num/len(y_test)\n",
    "print(\"Accuracy is %s.\" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model parameters\n",
    "Run this code to examine the model parameters you just learned. These parameters assign a postive or negative value to each word --- where positive values are words that tend to be spam and negative values are words that tend to be ham. Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 spam words\n",
      "  +2.5010  __currency__\n",
      "  +2.0928  call\n",
      "  +1.8753  txt\n",
      "  +1.8272  text\n",
      "  +1.7481  reply\n",
      "  +1.5347  service\n",
      "  +1.4760  150p\n",
      "  +1.4111  from\n",
      "  +1.4042  mobile\n",
      "  +1.3812  ringtone\n",
      "\n",
      "Top 10 ham words\n",
      "  -1.3479  my\n",
      "  -1.2551  so\n",
      "  -1.1009  ok\n",
      "  -1.0658  me\n",
      "  -0.9436  ll\n",
      "  -0.9252  later\n",
      "  -0.9207  what\n",
      "  -0.8735  he\n",
      "  -0.8693  come\n",
      "  -0.8036  still\n"
     ]
    }
   ],
   "source": [
    "token_weights = theta[1:]\n",
    "\n",
    "def reverse(a):\n",
    "    return a[::-1]\n",
    "\n",
    "most_negative = np.argsort(token_weights)\n",
    "most_positive = reverse(most_negative)\n",
    "\n",
    "k = 10\n",
    "\n",
    "print('Top %d spam words' % k)\n",
    "for i in most_positive[0:k]:\n",
    "    print('  %+.4f  %s' % (token_weights[i], tokens[i]))\n",
    "\n",
    "print('\\nTop %d ham words' % k)\n",
    "for i in most_negative[0:k]:\n",
    "    print('  %+.4f  %s' % (token_weights[i], tokens[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do they make sense?\n",
    "Yes, these top 10 spam/ham words make sense. The top spam words usually appear in the promotion texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on new messages\n",
    "Type a few of your own messages in below and make predictions. Are they ham or spam? Do the predictions make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5430165520801096\n",
      "0.016468406006459664\n",
      "0.42501882716868356\n"
     ]
    }
   ],
   "source": [
    "def extract_features(msg):\n",
    "    x = vectorizer.transform([msg]).toarray()\n",
    "    x = np.insert(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "msg1 = \"It's Donald Trump Jr. Dems just raised over $1OO MILLION to block my dad's Supreme Court nominee! Emergency 5x matching active http://teamtrump.co/24\"\n",
    "x1 = extract_features(msg1)  # this is the feature vector\n",
    "print(logistic(x1.dot(theta.T)))\n",
    "\n",
    "msg2 = \"hello how's it going\"\n",
    "x2 = extract_features(msg2)\n",
    "print(logistic(x2.dot(theta.T)))\n",
    "\n",
    "msg3 = \"Did you see Eric Trump's text? We still need YOU to complete your pledge to VOTE TRUMP! The deadline is tonight.\"\n",
    "x3 = extract_features(msg3)\n",
    "print(logistic(x3.dot(theta.T)))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#  - try a few texts of your own\n",
    "#  - predict whether they are spam or non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make a prediction on new messages.\n",
      "The first message is a spam. The predection based on our trained model is correct. \n",
      "The second message is a ham and the prediction is correct.\n",
      "The last message is a spam but the prediction shows that this is a ham, which is incorrect.\n"
     ]
    }
   ],
   "source": [
    "print(\"Make a prediction on new messages.\")\n",
    "print(\"The first message is a spam. The predection based on our trained model is correct. \")\n",
    "print(\"The second message is a ham and the prediction is correct.\")\n",
    "print(\"The last message is a spam but the prediction shows that this is a ham, which is incorrect.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
